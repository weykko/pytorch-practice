# Домашнее задание к уроку 3: Полносвязные сети

## Задание 1: Эксперименты с глубиной сети

### 1.1 Сравнение моделей разной глубины

Результаты обучения моделей с различным количеством слоев:

```
1_layer:
Итоговая точность train: 0.9274
Итоговая точность test: 0.9214
2_layers:
Итоговая точность train: 0.9940
Итоговая точность test: 0.9778
3_layers:
Итоговая точность train: 0.9929
Итоговая точность test: 0.9801
5_layers:
Итоговая точность train: 0.9907
Итоговая точность test: 0.9788
7_layers:
Итоговая точность train: 0.9895
Итоговая точность test: 0.9780
```

Кривые обучения 3-слойной модели:

![3_layers.png](plots%2F3_layers.png)

При увеличении количества слоев время обучения возрастает.  
Все результаты находятся в [depth_experiments.txt](results%2Fdepth_experiments.txt)

### 1.2 Анализ переобучения

Как видно все модели имеет большую точность на тренировочном наборе, это сигнализирует о переобучении моделей.  
3-слойная модель показала лучшую точность на тестовом наборе. Начиная с 2 слоёв, точность улучшается слабо, несмотря на резкий рост числа параметров.

Результаты обучения моделей с регуляризацией:

```
2_layers_reg:
Итоговая точность train: 0.9880
Итоговая точность test: 0.9789
3_layers_reg:
Итоговая точность train: 0.9867
Итоговая точность test: 0.9820
5_layers_reg:
Итоговая точность train: 0.9810
Итоговая точность test: 0.9830
7_layers_reg:
Итоговая точность train: 0.9776
Итоговая точность test: 0.9819
```

Кривые обучения 5-слойной модели с регуляризацией:

![5_layers_reg.png](plots%2F5_layers_reg.png)

Использование Dropout и BatchNorm уменьшает или полностью убирает переобучение моделей.

## Задание 2: Эксперименты с шириной сети

### 2.1 Сравнение моделей разной ширины

Результаты обучения моделей разной ширины:

```
narrow: Параметры: 53018, Время обучения: 265.00 с, Итоговая точность test: 0.9726
medium: Параметры: 242762, Время обучения: 271.69 с, Итоговая точность test: 0.9738
wide: Параметры: 1462538, Время обучения: 319.41 с, Итоговая точность test: 0.9795
very_wide: Параметры: 4235786, Время обучения: 388.86 с, Итоговая точность test: 0.9778
```

Лучшую точность показала модель шириной **wide**, хотя существенной разницы не наблюдается.  
Более широкие слои обучались дольше меньших.  
При увеличении ширины слоев соответственно возрастает и количество параметров модели. 

![width_experiment_results.png](plots%2Fwidth_experiment_results.png)

### 2.2 Оптимизация архитектуры

Результаты обучения моделей разной архитектуры:

```
Архитектура: contracting_512_256_128, Итоговая точность test: 0.9819
Архитектура: contracting_1024_512_256, Итоговая точность test: 0.9809
Архитектура: bottleneck_256_128_256, Итоговая точность test: 0.9807
Архитектура: expanding_256_512_1024, Итоговая точность test: 0.9806
Архитектура: bottleneck_512_256_512, Итоговая точность test: 0.9801
Архитектура: expanding_128_256_512, Итоговая точность test: 0.9799
Архитектура: constant_512, Итоговая точность test: 0.9777
Архитектура: constant_256, Итоговая точность test: 0.9769
```

Лучшую точность показала архитектура с сужением (512, 256, 128), но в целом результаты не сильно различаются.

![architecture_heatmap.png](plots%2Farchitecture_heatmap.png)

## Задание 3: Эксперименты с регуляризацией

### 3.1 Сравнение техник регуляризации

Результаты по всем техникам регуляризации находятся в [regularization_experiments.txt](results%2Fregularization_experiments.txt)

![weight_distribution.png](plots%2Fweight_distribution.png)

Техники регуляризации влияют на точность и стабильность обучения. Без регуляризации модель достигает высокой точности, но с риском переобучения. Dropout (0.1 и 0.3) немного снижает точность, а при 0.5 — значительно. BatchNorm обеспечивает стабильный рост точности. Комбинация Dropout (0.3) и BatchNorm улучшает результаты. L2 регуляризация дает устойчивые, но не лучшие результаты по сравнению с BatchNorm.

### 3.2 Адаптивная регуляризация

Результаты по всем адаптивным техникам регуляризации находятся в [regularization_experiments.txt](results%2Fregularization_experiments.txt)

Адаптивные техники регуляризации улучшили точность и стабильность. Адаптивный Dropout достиг 97.96% на тесте, BatchNorm с momentum 0.1 показал 98.30%, а с momentum 0.9 — 97.91%. Комбинированный подход достиг 98.08%. Все методы улучшили результаты по сравнению с обычным Dropout, подчеркивая эффективность адаптивных подходов.